{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise guides you through the process of classifying images using Support Vector Machines(SVM). As part of this you will:\n",
    "\n",
    "- Implement a fully vectorized loss function and for the SVM\n",
    "- Calculate the analytical gradient using vectorized code\n",
    "- Perform Stochastic Gradient Descent (SGD) to minimize the loss function\n",
    "- Validate to tune the hyper-parameters\n",
    "- Visualize the learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start-up code to import data-sets\n",
    "from f15ece6504.get_cifar10 import load_CIFAR10\n",
    "# loading some necessary packages\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is to make matplotlib figures appear inline in the\n",
    "# notebook rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some code so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Pre-processing CIFAR-10\n",
    "\n",
    "Before proceeding, go to `hw0/f15ece6504/data/` and run the script `.get_datasets.sh` to download the CIFAR-10 data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print('Training data shape: ', X_train.shape)? (<ipython-input-3-5b8811a418d4>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-5b8811a418d4>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    print 'Training data shape: ', X_train.shape\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print('Training data shape: ', X_train.shape)?\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have downloaded the CIFAR-10 database in your hw0/f15ece6504/f15ece6504/data folder, \n",
    "# we proceed to load the data into python\n",
    "cifar10_dir = 'f15ece6504/data/cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print 'Training labels shape: ', y_train.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample the data for more efficient code execution in this exercise.\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Our training set will be the first num_training points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "# As a sanity check, print out the shapes of the data\n",
    "print 'Training data shape: ', X_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Test data shape: ', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: subtract the mean image\n",
    "# first: compute the image mean based on the training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "print mean_image[:10] # print a few of the elements\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second: subtract the mean image from train and test data\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "# Also, lets transform both data matrices so that each image is a column.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))]).T\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))]).T\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))]).T\n",
    "# print X_train[0][-1]\n",
    "print X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier\n",
    "Code for this section will all be written inside `f15ece6504/classifiers/linear_svm.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from f15ece6504.classifiers.linear_svm import svm_loss_vectorized\n",
    "import time\n",
    "\n",
    "# generate a random SVM weight matrix of small numbers\n",
    "W = np.random.randn(10,3073) * 0.0001 \n",
    "\n",
    "# Implement a vectorized code to implement the loss and the gradient of the SVM. \n",
    "# Remember that your code should not have any loops. \n",
    "# Hint: \n",
    "# Look up the broadcasting property of np arrays\n",
    "# In case you have memory errors, you might want to use float16 or pickle your data\n",
    "# check np.ndarray.dump()/load() if you consider pickling your data\n",
    "                    \n",
    "tic = time.time()\n",
    "loss, grad = svm_loss_vectorized(W, X_train, y_train, 0.00001)\n",
    "toc = time.time()\n",
    "print 'Vectorized loss and gradient: computed in %fs' % (toc - tic)\n",
    "# It should be quite fast and finish within a second if your code is optimized! \n",
    "\n",
    "# check \n",
    "print 'loss is: ', loss\n",
    "# if your implementation is correct, loss should be around 9.0. \n",
    "\n",
    "# gradient check - The relative error between the numerical gradient and \n",
    "# your analytical gradient must be small \n",
    "from f15ece6504.gradient_check import grad_check_sparse\n",
    "f = lambda w: svm_loss_vectorized(w, X_train, y_train, 0.0)[0]\n",
    "grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "Code for this section will be written in `f15ece6504/classifiers/linear_classifier.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have efficient implementations for computing loss and gradients, let us use SGD to minimize \n",
    "# loss funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now implement SGD in LinearSVM.train() function and run it with the code below\n",
    "from f15ece6504.classifiers import LinearSVM\n",
    "svm = LinearSVM()\n",
    "tic = time.time()\n",
    "loss_hist = svm.train(X_train, y_train, learning_rate=1e-7, reg=5e4,\n",
    "                      num_iters=1500, verbose=True)\n",
    "toc = time.time()\n",
    "print 'That took %fs' % (toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A useful debugging strategy is to plot the training loss as a function of\n",
    "# iteration number:\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the LinearSVM.predict function and evaluate the performance on both the\n",
    "# training and validation set\n",
    "y_train_pred = svm.predict(X_train)\n",
    "print 'training accuracy: %f' % (np.mean(y_train == y_train_pred), )\n",
    "y_val_pred = svm.predict(X_val)\n",
    "print 'validation accuracy: %f' % (np.mean(y_val == y_val_pred), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the parameters\n",
    "In our classifier, the learning rate and the regularization strength are hyper-parameters. Tuning them to appropriate values is needed to bring out the best from these classifiers. Generally, this takes experience and is generally considered a dark art! Spearmint is a software package which does this using Bayesian Optimization.\n",
    "\n",
    "Assuming you have installed spearmint as mentioned in the README, we proceed to its set-up and usage. Spearmint needs you to create two files:\n",
    "1. A wrapper file that calls the classifier, trains it and returns the error on the validation set \n",
    "   - In spearmint-tuner.py, look at the function get_valError()\n",
    "2. A config file that has details about the parameters to be tuned, language, etc. \n",
    "   - config.json - Note that this contains the minimum and maximum values of the parameters to be tuned. You will        have to adjust this based on intuition. A smaller search range generally implies faster convergence. \n",
    "\n",
    "In the Spearmint installation directory, you have many examples that should give you an idea. You are provided with these two files in the `VT-F15-ECE6504-HW0` directory.\n",
    "\n",
    "Depending on the classifier for which you are tuning, make the following changes:\n",
    "1. Uncomment the appropriate lines in the spearmint-tuner.py file\n",
    "2. Modify the min and max fields in the config.json file.\n",
    "\n",
    "After these modifications, execute the below commands\n",
    "\n",
    "```sh\n",
    "$ mongod --fork --logpath <path/to/logfile\\> --dbpath <path/to/dbfolder\\>\n",
    "$ python /path/to/spearmint_install/spearmint/main.py </path/to/hw0\\>\n",
    "```\n",
    "\n",
    "You may need to make a directory i.e. `dbfolder` before running the above commands. \n",
    "\n",
    "Once you tune the model, use these parameters to retrain the classifier (modify and run the first cell in the SGD section and proceed to the next). Spearmint saves the output of each iteration in `hw0/output`\n",
    "\n",
    "Note that spearmint searches indefinitely. It reaches the near-optimal solution and oscillates in that region. You are required to stop the process when the validation error does not show any significant decrease. If your parameters are properly tuned, you will get about ~37% accuracy on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class.\n",
    "# Depending on your choice of learning rate and regularization strength, these may\n",
    "# or may not be nice to look at.\n",
    "w = svm.W[:,:-1] # strip out the bias\n",
    "w = w.reshape(10, 32, 32, 3)\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
