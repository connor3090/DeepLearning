{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise guides you through the process of classifying images using Logistic Regression/Softmax Classifier. As part of this you will:\n",
    "\n",
    "- Implement a fully vectorized loss function for the Softmax Classifier\n",
    "- Calculate the analytical gradient using vectorized code\n",
    "- Use Validation to tune the hyper-parameters\n",
    "- Perform Stochastic Gradient Descent (SGD) to minimize the loss function\n",
    "- Visualize the learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start-up code! \n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from load_cifar10_tvt import load_cifar10_train_val\n",
    "X_train, y_train, X_test, y_test = load_cifar10_train_val()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for this section is to be written in `f15ece6504/classifiers/softmax.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, implement the vectorized version in softmax_loss_vectorized.\n",
    "\n",
    "import time\n",
    "from f15ece6504.classifiers.softmax import softmax_loss_vectorized\n",
    "\n",
    "W = np.random.randn(10, 3073) * 0.0001\n",
    "\n",
    "tic = time.time()\n",
    "loss, grad = softmax_loss_vectorized(W, X_train, y_train, 0.00001)\n",
    "toc = time.time()\n",
    "print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)\n",
    "\n",
    "# This should also be quite fast! \n",
    "\n",
    "As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print 'loss: %f' % loss\n",
    "print 'sanity check: %f' % (-np.log(0.1))\n",
    "\n",
    "# gradient check. \n",
    "from f15ece6504.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_vectorized(w, X_train, y_train, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that efficient implementations to calculate loss function and gradient of the softmax are ready,\n",
    "# use it to train the classifier on the cifar-10 data\n",
    "from f15ece6504.classifiers import Softmax\n",
    "classifier = Softmax()\n",
    "loss_hist = classifier.train(X_train, y_train, learning_rate=1e-3, reg=1e-5, num_iters=100,batch_size=200, verbose=False)\n",
    "# plot loss vs. iterations as before\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best svm on test set\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before visualizing the weights learned, tune the model with spearmint! Modify and re-train the Softmax classifier in the previous cell before you proceed. You can modify the file used to tune the SVM and modify it by calling the Softmax classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = classifier.W[:,:-1] # strip out the bias\n",
    "w = w.reshape(10, 32, 32, 3)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
